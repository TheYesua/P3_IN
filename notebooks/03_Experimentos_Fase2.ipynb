{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Experimentos Fase 2 - Optimización de Modelos\n",
    "\n",
    "Este notebook documenta los experimentos realizados en la Fase 2 para mejorar el F1-score.\n",
    "\n",
    "**Experimentos incluidos:**\n",
    "- Exp07: Logistic Regression optimizado (GridSearchCV)\n",
    "- Exp08: VotingClassifier ensemble\n",
    "- Exp09: StackingClassifier\n",
    "- Exp10: SVM optimizado\n",
    "- Exp11: LightGBM regularizado\n",
    "- Exp12: LR con calibración y umbral óptimo\n",
    "- Exp13: Voting LR + SVM\n",
    "- Exp14: Variantes de LR (L1, L2, ElasticNet, Ridge)\n",
    "- Exp15: Ridge Classifier optimizado\n",
    "\n",
    "**Gráficas generadas:**\n",
    "1. Comparación de F1-scores Fase 2\n",
    "2. Comparación completa (Fase 1 + Fase 2)\n",
    "3. Matriz de confusión del mejor modelo (Ridge)\n",
    "4. Análisis de regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuración del proyecto\n",
    "PROJECT_ROOT = Path(os.getcwd()).parent if Path(os.getcwd()).name == 'notebooks' else Path(os.getcwd())\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f'PROJECT_ROOT: {PROJECT_ROOT}')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de estilo\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "from src.preprocessing import load_data, prepare_features, scale_features\n",
    "from src.models import evaluate_model_cv\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = PROJECT_ROOT / 'data'\n",
    "GRAFICAS_PATH = PROJECT_ROOT / 'docs' / 'graficas'\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"✓ Librerías cargadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "train_df, test_df = load_data(DATA_PATH / 'train.csv', DATA_PATH / 'test.csv')\n",
    "X_train, X_test, y_train, feature_cols = prepare_features(train_df, test_df)\n",
    "X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)\n",
    "\n",
    "print(f\"Muestras de entrenamiento: {X_train.shape[0]}\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"Distribución de clases: {dict(zip(*np.unique(y_train, return_counts=True)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluación de Modelos Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fase2 = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Exp07: LR optimizado\n",
    "lr_opt = LogisticRegression(C=5.0, solver='liblinear', penalty='l2', max_iter=2000, random_state=RANDOM_STATE)\n",
    "m, s, _ = evaluate_model_cv(lr_opt, X_train_scaled, y_train)\n",
    "results_fase2.append({'Experimento': 'Exp07: LR Optimizado', 'F1_Mean': m, 'F1_Std': s})\n",
    "print(f\"Exp07 - LR Optimizado (C=5): F1 = {m:.4f} (+/- {s:.4f})\")\n",
    "\n",
    "# Exp08: Voting Ensemble\n",
    "estimators_voting = [\n",
    "    ('lr', LogisticRegression(max_iter=1000, C=1.0, random_state=RANDOM_STATE)),\n",
    "    ('svm', SVC(kernel='rbf', C=1.0, probability=True, random_state=RANDOM_STATE)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_STATE)),\n",
    "    ('nb', GaussianNB()),\n",
    "]\n",
    "voting = VotingClassifier(estimators=estimators_voting, voting='hard')\n",
    "m, s, _ = evaluate_model_cv(voting, X_train_scaled, y_train)\n",
    "results_fase2.append({'Experimento': 'Exp08: Voting Hard', 'F1_Mean': m, 'F1_Std': s})\n",
    "print(f\"Exp08 - Voting Hard: F1 = {m:.4f} (+/- {s:.4f})\")\n",
    "\n",
    "# Exp09: Stacking\n",
    "estimators_stack = [\n",
    "    ('lr', LogisticRegression(max_iter=1000, C=1.0, random_state=RANDOM_STATE)),\n",
    "    ('svm', SVC(kernel='rbf', C=1.0, probability=True, random_state=RANDOM_STATE)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, max_depth=10, random_state=RANDOM_STATE)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
    "    ('nb', GaussianNB()),\n",
    "]\n",
    "stacking = StackingClassifier(estimators=estimators_stack, final_estimator=LogisticRegression(max_iter=1000, random_state=RANDOM_STATE), cv=5)\n",
    "m, s, _ = evaluate_model_cv(stacking, X_train_scaled, y_train)\n",
    "results_fase2.append({'Experimento': 'Exp09: Stacking', 'F1_Mean': m, 'F1_Std': s})\n",
    "print(f\"Exp09 - Stacking: F1 = {m:.4f} (+/- {s:.4f})\")\n",
    "\n",
    "# Exp10: SVM optimizado\n",
    "svm_opt = SVC(C=1.0, kernel='linear', random_state=RANDOM_STATE)\n",
    "m, s, _ = evaluate_model_cv(svm_opt, X_train_scaled, y_train)\n",
    "results_fase2.append({'Experimento': 'Exp10: SVM Linear', 'F1_Mean': m, 'F1_Std': s})\n",
    "print(f\"Exp10 - SVM Linear: F1 = {m:.4f} (+/- {s:.4f})\")\n",
    "\n",
    "# Exp14: Ridge Classifier\n",
    "ridge = RidgeClassifier(alpha=1.0, random_state=RANDOM_STATE)\n",
    "m, s, _ = evaluate_model_cv(ridge, X_train_scaled, y_train)\n",
    "results_fase2.append({'Experimento': 'Exp15: Ridge (α=1)', 'F1_Mean': m, 'F1_Std': s})\n",
    "print(f\"Exp15 - Ridge (α=1): F1 = {m:.4f} (+/- {s:.4f})\")\n",
    "\n",
    "results_fase2_df = pd.DataFrame(results_fase2)\n",
    "results_fase2_df.sort_values('F1_Mean', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gráfica: Comparación de Modelos Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "results_sorted = results_fase2_df.sort_values('F1_Mean', ascending=True)\n",
    "colors = ['#e74c3c' if x < 0.78 else '#f39c12' if x < 0.84 else '#27ae60' for x in results_sorted['F1_Mean']]\n",
    "\n",
    "bars = ax.barh(results_sorted['Experimento'], results_sorted['F1_Mean'], color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.errorbar(results_sorted['F1_Mean'], results_sorted['Experimento'], xerr=results_sorted['F1_Std'], fmt='none', color='black', capsize=3)\n",
    "\n",
    "for bar, score in zip(bars, results_sorted['F1_Mean']):\n",
    "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, f'{score:.4f}', va='center', fontsize=10)\n",
    "\n",
    "# Línea del baseline original\n",
    "ax.axvline(x=0.8278, color='blue', linestyle='--', linewidth=2, label='Baseline Fase 1: 0.8278')\n",
    "\n",
    "ax.set_xlabel('F1-Score (Validación Cruzada 5-fold)')\n",
    "ax.set_title('Comparación de Modelos - Fase 2 (Optimización)')\n",
    "ax.set_xlim(0.65, 0.95)\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAFICAS_PATH / 'comparacion_modelos_fase2.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Guardada: comparacion_modelos_fase2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparación Completa: Fase 1 + Fase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados Fase 1\n",
    "results_fase1 = [\n",
    "    {'Experimento': 'Exp01: Baseline LR', 'F1_Mean': 0.8278, 'F1_Std': 0.0232, 'Fase': 'Fase 1'},\n",
    "    {'Experimento': 'Exp02: PCA(3) + LR', 'F1_Mean': 0.7368, 'F1_Std': 0.0020, 'Fase': 'Fase 1'},\n",
    "    {'Experimento': 'Exp03: PCA(7) + XGB', 'F1_Mean': 0.7268, 'F1_Std': 0.0508, 'Fase': 'Fase 1'},\n",
    "    {'Experimento': 'Exp04: SKB(50) + RF', 'F1_Mean': 0.6802, 'F1_Std': 0.0544, 'Fase': 'Fase 1'},\n",
    "    {'Experimento': 'Exp05: XGBoost', 'F1_Mean': 0.7472, 'F1_Std': 0.0520, 'Fase': 'Fase 1'},\n",
    "    {'Experimento': 'Exp06: LightGBM', 'F1_Mean': 0.7232, 'F1_Std': 0.0556, 'Fase': 'Fase 1'},\n",
    "]\n",
    "\n",
    "# Añadir fase a resultados fase 2\n",
    "for r in results_fase2:\n",
    "    r['Fase'] = 'Fase 2'\n",
    "\n",
    "# Combinar\n",
    "all_results = pd.DataFrame(results_fase1 + results_fase2)\n",
    "all_results_sorted = all_results.sort_values('F1_Mean', ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "colors = ['#3498db' if fase == 'Fase 1' else '#9b59b6' for fase in all_results_sorted['Fase']]\n",
    "\n",
    "bars = ax.barh(all_results_sorted['Experimento'], all_results_sorted['F1_Mean'], color=colors, edgecolor='black', linewidth=0.5)\n",
    "ax.errorbar(all_results_sorted['F1_Mean'], all_results_sorted['Experimento'], xerr=all_results_sorted['F1_Std'], fmt='none', color='black', capsize=3)\n",
    "\n",
    "for bar, score in zip(bars, all_results_sorted['F1_Mean']):\n",
    "    ax.text(bar.get_width() + 0.005, bar.get_y() + bar.get_height()/2, f'{score:.4f}', va='center', fontsize=9)\n",
    "\n",
    "# Leyenda personalizada\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='#3498db', edgecolor='black', label='Fase 1 (Exploración)'),\n",
    "    Patch(facecolor='#9b59b6', edgecolor='black', label='Fase 2 (Optimización)')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "ax.set_xlabel('F1-Score (Validación Cruzada 5-fold)')\n",
    "ax.set_title('Comparación Completa de Todos los Experimentos')\n",
    "ax.set_xlim(0.6, 0.95)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAFICAS_PATH / 'comparacion_modelos_completa.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Guardada: comparacion_modelos_completa.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matriz de Confusión: Mejor Modelo (Ridge Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor modelo: Ridge Classifier\n",
    "best_model = RidgeClassifier(alpha=1.0, random_state=RANDOM_STATE)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "y_pred = cross_val_predict(best_model, X_train_scaled, y_train, cv=skf)\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "f1 = f1_score(y_train, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=ax,\n",
    "            xticklabels=['Control (sana)', 'Botrytis (infectada)'],\n",
    "            yticklabels=['Control (sana)', 'Botrytis (infectada)'],\n",
    "            annot_kws={'size': 16})\n",
    "\n",
    "ax.set_title(f'Matriz de Confusión - Ridge Classifier\\nF1-Score = {f1:.4f}', fontsize=14)\n",
    "ax.set_xlabel('Predicción', fontsize=12)\n",
    "ax.set_ylabel('Valor Real', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAFICAS_PATH / 'matriz_confusion_ridge.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Guardada: matriz_confusion_ridge.png\")\n",
    "\n",
    "# Métricas detalladas\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nMétricas detalladas:\")\n",
    "print(f\"  - Verdaderos Negativos (TN): {tn}\")\n",
    "print(f\"  - Falsos Positivos (FP): {fp}\")\n",
    "print(f\"  - Falsos Negativos (FN): {fn}\")\n",
    "print(f\"  - Verdaderos Positivos (TP): {tp}\")\n",
    "print(f\"  - Precision: {tp/(tp+fp):.4f}\")\n",
    "print(f\"  - Recall: {tp/(tp+fn):.4f}\")\n",
    "print(f\"  - F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análisis de Regularización: Ridge vs Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar diferentes valores de regularización\n",
    "alphas_ridge = [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "Cs_lr = [100, 10, 2, 1, 0.5, 0.2, 0.1]  # C = 1/alpha para comparación\n",
    "\n",
    "ridge_scores = []\n",
    "lr_scores = []\n",
    "\n",
    "for alpha, C in zip(alphas_ridge, Cs_lr):\n",
    "    # Ridge\n",
    "    ridge = RidgeClassifier(alpha=alpha, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(ridge, X_train_scaled, y_train, cv=cv, scoring='f1')\n",
    "    ridge_scores.append({'alpha': alpha, 'F1_Mean': scores.mean(), 'F1_Std': scores.std()})\n",
    "    \n",
    "    # LR\n",
    "    lr = LogisticRegression(C=C, solver='liblinear', max_iter=2000, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(lr, X_train_scaled, y_train, cv=cv, scoring='f1')\n",
    "    lr_scores.append({'C': C, 'F1_Mean': scores.mean(), 'F1_Std': scores.std()})\n",
    "\n",
    "ridge_df = pd.DataFrame(ridge_scores)\n",
    "lr_df = pd.DataFrame(lr_scores)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Ridge\n",
    "ax1.errorbar(ridge_df['alpha'], ridge_df['F1_Mean'], yerr=ridge_df['F1_Std'], \n",
    "             fmt='o-', color='#27ae60', linewidth=2, markersize=8, capsize=4)\n",
    "ax1.set_xlabel('Alpha (regularización)', fontsize=12)\n",
    "ax1.set_ylabel('F1-Score', fontsize=12)\n",
    "ax1.set_title('Ridge Classifier: Efecto de Alpha', fontsize=14)\n",
    "ax1.set_xscale('log')\n",
    "ax1.axhline(y=ridge_df['F1_Mean'].max(), color='red', linestyle='--', alpha=0.5)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# LR\n",
    "ax2.errorbar(lr_df['C'], lr_df['F1_Mean'], yerr=lr_df['F1_Std'],\n",
    "             fmt='s-', color='#3498db', linewidth=2, markersize=8, capsize=4)\n",
    "ax2.set_xlabel('C (inverso de regularización)', fontsize=12)\n",
    "ax2.set_ylabel('F1-Score', fontsize=12)\n",
    "ax2.set_title('Logistic Regression: Efecto de C', fontsize=14)\n",
    "ax2.set_xscale('log')\n",
    "ax2.axhline(y=lr_df['F1_Mean'].max(), color='red', linestyle='--', alpha=0.5)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(GRAFICAS_PATH / 'analisis_regularizacion.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"✓ Guardada: analisis_regularizacion.png\")\n",
    "\n",
    "print(\"\\nMejor Ridge:\", ridge_df.loc[ridge_df['F1_Mean'].idxmax()])\n",
    "print(\"Mejor LR:\", lr_df.loc[lr_df['F1_Mean'].idxmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tabla Resumen de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla completa con resultados de Kaggle (actualizar manualmente)\n",
    "resultados_kaggle = {\n",
    "    'Exp01: Baseline LR': 0.84782,\n",
    "    'Exp02: PCA(3) + LR': 0.78378,\n",
    "    'Exp03: PCA(7) + XGB': 0.73033,\n",
    "    'Exp04: SKB(50) + RF': 0.70270,\n",
    "    'Exp05: XGBoost': 0.78378,\n",
    "    'Exp06: LightGBM': 0.81081,\n",
    "    # Fase 2 - Pendiente de actualizar\n",
    "    'Exp15: Ridge (α=1)': None,  # PENDIENTE\n",
    "}\n",
    "\n",
    "# Crear tabla resumen\n",
    "tabla_resumen = all_results.copy()\n",
    "tabla_resumen['F1_Kaggle'] = tabla_resumen['Experimento'].map(resultados_kaggle)\n",
    "tabla_resumen['Gap'] = tabla_resumen['F1_Kaggle'] - tabla_resumen['F1_Mean']\n",
    "tabla_resumen = tabla_resumen.sort_values('F1_Mean', ascending=False)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TABLA RESUMEN DE EXPERIMENTOS\")\n",
    "print(\"=\"*80)\n",
    "print(tabla_resumen.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resumen de Gráficas Generadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gráficas generadas en:\", GRAFICAS_PATH)\n",
    "print(\"\\nArchivos nuevos de Fase 2:\")\n",
    "nuevas_graficas = ['comparacion_modelos_fase2.png', 'comparacion_modelos_completa.png', \n",
    "                   'matriz_confusion_ridge.png', 'analisis_regularizacion.png']\n",
    "for f in nuevas_graficas:\n",
    "    path = GRAFICAS_PATH / f\n",
    "    if path.exists():\n",
    "        print(f\"  ✓ {f}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {f} (no generada)\")\n",
    "\n",
    "print(\"\\n✓ Listo para incluir en el documento LaTeX!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
