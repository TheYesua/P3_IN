# Pr√≥ximos Pasos - P3 Kaggle

## Estado Actual
- **Score actual**: 0.8478 (posici√≥n 1 de 2)
- **Modelo baseline**: Logistic Regression + StandardScaler
- **F1-score CV**: 0.8278, **F1-score Kaggle**: 0.8478

---

## Estrategias para Mejorar el Score

### 1. Reducci√≥n de Dimensionalidad (Prioridad: Alta)
**Raz√≥n**: El EDA mostr√≥ que 3 componentes PCA explican el 95% de la varianza.

**Experimentos propuestos**:
- `exp_02_pca_3components.py`: PCA con 3 componentes + Logistic Regression
- `exp_03_pca_7components.py`: PCA con 7 componentes + Random Forest
- `exp_04_selectkbest.py`: SelectKBest con k=50 + XGBoost

**Expected improvement**: +0.01-0.03 en F1-score

### 2. Modelos Avanzados (Prioridad: Alta)
**Algoritmos a probar**:
- **XGBoost**: Excelente para datos tabulares, robusto al overfitting
- **LightGBM**: M√°s r√°pido que XGBoost, similar rendimiento
- **CatBoost**: Manejo autom√°tico de variables categ√≥ricas (no aplica aqu√≠ pero bueno para comparar)

**Configuraci√≥n base sugerida**:
```python
XGBClassifier(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
```

### 3. Optimizaci√≥n de Hiperpar√°metros (Prioridad: Media)
**Herramienta**: Optuna
**Modelos a optimizar**:
- Logistic Regression (C, penalty, solver)
- XGBoost (n_estimators, max_depth, learning_rate, etc.)
- Random Forest (n_estimators, max_depth, min_samples_split)

### 4. T√©cnicas de Ensemble (Prioridad: Media)
**M√©todos**:
- **Voting Classifier**: Combinar predicciones de m√∫ltiples modelos
- **Stacking**: Usar predicciones de modelos base como features para un meta-modelo
- **Bagging**: Multiple instancias del mismo modelo con diferentes subsets

### 5. Feature Engineering (Prioridad: Baja-Media)
**Ideas**:
- **Polynomial features**: Interacciones entre variables de fluorescencia
- **Spectral indices**: Ratios entre bandas espectrales espec√≠ficas
- **Domain knowledge**: √çndices vegetativos (NDVI-like) adaptados a fluorescencia

---

## Plan de Ejecuci√≥n Sugerido

### Fase 1: Reducci√≥n Dimensionalidad (1-2 d√≠as)
1. Implementar experimentos con PCA
2. Evaluar impacto en diferentes modelos
3. Seleccionar mejor configuraci√≥n de dimensionalidad

### Fase 2: Modelos Avanzados (2-3 d√≠as)
1. Implementar XGBoost baseline
2. Probar LightGBM y CatBoost
3. Comparar con Logistic Regression

### Fase 3: Optimizaci√≥n (2-3 d√≠as)
1. Configurar Optuna para b√∫squeda de hiperpar√°metros
2. Optimizar top 2 modelos
3. Validar con cross-validation estratificado

### Fase 4: Ensemble y Finalizaci√≥n (1-2 d√≠as)
1. Implementar Voting Classifier con mejores modelos
2. Probar Stacking si el tiempo lo permite
3. Generar submission final

---

## Scripts a Crear

```
scripts/
‚îú‚îÄ‚îÄ exp_02_pca_logistic.py
‚îú‚îÄ‚îÄ exp_03_pca_xgboost.py
‚îú‚îÄ‚îÄ exp_04_selectkbest_rf.py
‚îú‚îÄ‚îÄ exp_05_xgboost_baseline.py
‚îú‚îÄ‚îÄ exp_06_lightgbm_baseline.py
‚îú‚îÄ‚îÄ exp_07_optuna_xgboost.py
‚îú‚îÄ‚îÄ exp_08_optuna_rf.py
‚îî‚îÄ‚îÄ exp_09_ensemble_voting.py
```

---

## M√©tricas de Progreso

**Objetivos intermedios**:
- Superar 0.85: ‚úÖ (actual: 0.8478)
- Alcanzar 0.86: üéØ (pr√≥ximo objetivo)
- Superar 0.87: üöÄ (optimista)
- Llegar a 0.88: üèÜ (muy ambicioso)

**Deadline sugerido**: 1-2 semanas para implementar y probar las mejoras principales.

---

## Notas Importantes

- **Semillas aleatorias**: Mantener `random_state=42` para reproducibilidad
- **Validaci√≥n cruzada**: Usar siempre StratifiedKFold(n_splits=5)
- **Documentaci√≥n**: Actualizar `REGISTRO_EXPERIMENTOS.md` despu√©s de cada submission
- **Backups**: Guardar modelos y predicciones de cada experimento
- **Tiempo**: Priorizar experimentos con mayor potencial de mejora

---

*√öltima actualizaci√≥n: 23/12/2025*
