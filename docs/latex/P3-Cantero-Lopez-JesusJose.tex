\documentclass[11pt]{article}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{xcolor}

\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    frame=single,
    breaklines=true,
    columns=fullflexible
}

\setlength{\parindent}{0mm}
\setlength{\parskip}{1em}
\pagestyle{fancy}
\setlength{\headheight}{14pt}
\rhead{Inteligencia de Negocio}
\lhead{Práctica 3}
\renewcommand{\footrulewidth}{0.5pt}
\fancyfoot[L]{Grado en Ingeniería Informática}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\hyperref[sec:indice]{Índice}}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{\fill}
    {\Huge\bfseries Práctica 3 \\[0.5cm]}
    {\LARGE Competición Kaggle: Clasificación de Hojas de Tomate \\[0.5cm]}
    {\Large Inteligencia de Negocio \\[2cm]}
    {\large Jesús J. Cantero \\[0.3cm]}
    {\large Grupo A \\[0.3cm]}
    {\large jesusjcl@correo.ugr.es \\[2cm]}
    {\large Curso 2025--2026 \\[0.5cm]}
    \vspace*{\fill}
    {\large Grado en Ingeniería Informática \\}
    {\large Universidad de Granada (Ceuta)}
\end{titlepage}

% Página obligatoria: Captura del Leaderboard de Kaggle
\thispagestyle{empty}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../capturas/leaderboard_231225.png}
    \caption*{\textbf{Captura del Leaderboard de Kaggle} -- Fecha: 23/12/2025}
\end{figure}
\vspace{1cm}
\begin{center}
    \textbf{Resumen de participación:}\\[0.5cm]
    \begin{tabular}{ll}
        \textbf{Usuario:} & JesusCantero\_UGR\_IN \\
        \textbf{Posición actual:} & 1 de 2 \\
        \textbf{Mejor Score:} & 0.84782 \\
        \textbf{Número de entries:} & 6 \\
    \end{tabular}
\end{center}
\clearpage

\tableofcontents
\label{sec:indice}
\clearpage

% 1. Introducción
\section{Introducción}

\subsection{Contexto y Motivación}

Esta práctica consiste en una competición Kaggle de clasificación binaria de hojas de tomate. El objetivo es distinguir entre hojas sanas (\textit{control}) y hojas infectadas por el hongo \textit{Botrytis cinerea} (\textit{botrytis}) utilizando datos de fluorescencia multicolor e imágenes hiperespectrales.

La detección temprana de enfermedades en cultivos es fundamental para la agricultura de precisión y la gestión eficiente de recursos. Las técnicas de aprendizaje automático permiten automatizar el diagnóstico a partir de mediciones no destructivas, reduciendo pérdidas y optimizando el uso de tratamientos fitosanitarios.

\subsection{Descripción del Dataset}

El dataset proporcionado contiene mediciones de hojas de tomate:

\begin{itemize}
    \item \textbf{Conjunto de entrenamiento}: 336 muestras con etiquetas
    \item \textbf{Conjunto de test}: 143 muestras sin etiquetas
    \item \textbf{Total de variables}: 309 columnas
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{lll}
        \toprule
        \textbf{Tipo} & \textbf{Columnas} & \textbf{Descripción} \\
        \midrule
        Metadatos (NO USAR) & \texttt{exp}, \texttt{dpi}, \texttt{leaf}, \texttt{spot} & Información experimental \\
        Fluorescencia & \texttt{F440}, \texttt{F520}, \texttt{F680}, \texttt{F740} & 4 valores de fluorescencia \\
        Hiperespectral & \texttt{w388.13} a \texttt{w1028.28} & 300 variables espectrales \\
        Target & \texttt{class} & \texttt{control} (0) o \texttt{botrytis} (1) \\
        \bottomrule
    \end{tabular}
    \caption{Descripción de las variables del dataset.}
    \label{tab:variables}
\end{table}

\subsection{Herramientas y Tecnologías}

\begin{itemize}
    \item \textbf{pandas} y \textbf{numpy}: manipulación de datos
    \item \textbf{scikit-learn}: modelos de clasificación, escalado y validación cruzada
    \item \textbf{XGBoost} y \textbf{LightGBM}: algoritmos de gradient boosting
    \item \textbf{matplotlib} y \textbf{seaborn}: visualización
    \item \textbf{Jupyter Notebook}: análisis exploratorio interactivo
\end{itemize}

\clearpage
\subsection{Métrica de Evaluación}

La métrica utilizada en la competición es el \textbf{F1-score}, definido como:

\[
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]

Esta métrica es apropiada para problemas de clasificación binaria con posible desbalance de clases, ya que considera tanto los falsos positivos como los falsos negativos.

\clearpage
% 2. Análisis Exploratorio de Datos
\section{Análisis Exploratorio de Datos (EDA)}

\subsection{Distribución de Clases}

El análisis de la distribución de clases revela un desbalance moderado:

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Clase} & \textbf{Muestras} & \textbf{Porcentaje} \\
        \midrule
        botrytis & 196 & 58.3\% \\
        control & 140 & 41.7\% \\
        \bottomrule
    \end{tabular}
    \caption{Distribución de clases en el conjunto de entrenamiento.}
    \label{tab:distribucion}
\end{table}

El ratio de desbalance es de 1.40:1, lo cual es moderado y no requiere técnicas agresivas de balanceo.

\subsection{Variables de Fluorescencia}

Las 4 variables de fluorescencia presentan las siguientes características:

\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Variable} & \textbf{Media} & \textbf{Std} & \textbf{Min} & \textbf{Max} \\
        \midrule
        F440 & 235.57 & 9.19 & 209.95 & 257.33 \\
        F520 & 431.88 & 31.66 & 327.26 & 517.98 \\
        F680 & 773.10 & 120.98 & 484.27 & 1270.81 \\
        F740 & 1403.55 & 227.24 & 795.12 & 2125.33 \\
        \bottomrule
    \end{tabular}
    \caption{Estadísticas descriptivas de las variables de fluorescencia.}
    \label{tab:fluorescencia}
\end{table}

Se observan dos grupos de variables correlacionadas:
\begin{itemize}
    \item \textbf{Grupo 1}: F440 y F520 (correlación positiva: 0.89)
    \item \textbf{Grupo 2}: F680 y F740 (correlación positiva: 0.96)
\end{itemize}

Existe correlación negativa entre ambos grupos, sugiriendo características complementarias.

\subsection{Variables Espectrales}

El dataset incluye 300 variables espectrales en el rango de 388.13 nm a 1028.28 nm. El análisis de espectros promedio por clase muestra diferencias sutiles pero consistentes entre hojas sanas e infectadas.

\subsection{Análisis de Valores Faltantes y Outliers}

\begin{itemize}
    \item \textbf{Valores faltantes}: No se detectaron valores faltantes en ninguno de los conjuntos.
    \item \textbf{Outliers} (método IQR en fluorescencia):
    \begin{itemize}
        \item F440: 4 outliers (1.2\%)
        \item F520: 10 outliers (3.0\%)
        \item F680: 6 outliers (1.8\%)
        \item F740: 4 outliers (1.2\%)
    \end{itemize}
\end{itemize}

\subsection{Reducción de Dimensionalidad (PCA)}

El análisis de componentes principales revela alta reducibilidad dimensional:

\begin{itemize}
    \item \textbf{3 componentes} explican el 95\% de la varianza
    \item \textbf{7 componentes} explican el 99\% de la varianza
\end{itemize}

Esto indica que, a pesar de tener 304 features, la información relevante se concentra en pocas dimensiones.

\clearpage
% 3. Preprocesamiento
\section{Preprocesamiento de Datos}

\subsection{Limpieza de Datos}

Durante la carga de datos se detectó un problema de formato: algunos valores numéricos contenían espacios (ej: \texttt{'232 .25'}). Se implementó una función de limpieza automática:

\begin{lstlisting}
def clean_numeric_columns(df):
    for col in df.columns:
        if df[col].dtype == 'object' and col not in METADATA_COLS + [TARGET_COL]:
            df[col] = df[col].astype(str).str.replace(' ', '').astype(float)
    return df
\end{lstlisting}

\subsection{Preparación de Features}

El pipeline de preprocesamiento incluye:

\begin{enumerate}
    \item \textbf{Exclusión de metadatos}: Se eliminan las columnas \texttt{exp}, \texttt{dpi}, \texttt{leaf}, \texttt{spot}
    \item \textbf{Codificación del target}: \texttt{control} $\rightarrow$ 0, \texttt{botrytis} $\rightarrow$ 1
    \item \textbf{Escalado}: StandardScaler para normalizar las features
\end{enumerate}

\subsection{Arquitectura del Código}

El código se organiza en módulos reutilizables:

\begin{itemize}
    \item \texttt{src/preprocessing.py}: Funciones de carga, limpieza y transformación
    \item \texttt{src/models.py}: Definición y evaluación de modelos
    \item \texttt{src/utils.py}: Utilidades para submissions y logging
\end{itemize}

\clearpage
% 4. Modelado
\section{Modelado y Experimentación}

\subsection{Metodología}

Se utilizó validación cruzada estratificada con 5 folds para evaluar los modelos, asegurando que cada fold mantiene la proporción de clases del dataset original.

\subsection{Experimento 01: Modelos Baseline}

Se compararon 5 algoritmos de clasificación con parámetros por defecto:

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Modelo} & \textbf{F1-Score (CV)} & \textbf{Desv. Std} \\
        \midrule
        Logistic Regression & \textbf{0.9388} & 0.0254 \\
        SVM (RBF) & 0.9326 & -- \\
        Random Forest & 0.9266 & -- \\
        Gradient Boosting & 0.9203 & -- \\
        KNN (k=5) & 0.8943 & -- \\
        \bottomrule
    \end{tabular}
    \caption{Comparación de modelos baseline con StandardScaler.}
    \label{tab:baseline}
\end{table}

\textbf{Resultado}: Logistic Regression obtuvo el mejor F1-score en validación cruzada (0.9388), siendo seleccionado para la primera submission.

\subsection{Configuración del Mejor Modelo}

\begin{lstlisting}
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Preprocesamiento
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Modelo
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_scaled, y_train)

# Predicciones
predictions = model.predict(X_test_scaled)
\end{lstlisting}

\clearpage
% 5. Registro de Experimentos
\section{Registro de Experimentos}

La siguiente tabla contiene el registro obligatorio de todas las submissions realizadas a Kaggle, incluyendo fecha/hora, posición en el momento de la subida, scores de entrenamiento y test, preprocesado aplicado, algoritmo utilizado y configuración de parámetros.

\begin{table}[H]
    \centering
    \footnotesize
    \begin{tabular}{|c|c|c|c|c|p{2.5cm}|p{2.5cm}|p{3cm}|}
        \hline
        \textbf{Nº} & \textbf{Fecha/Hora} & \textbf{Pos.} & \textbf{F1 Train} & \textbf{F1 Kaggle} & \textbf{Preprocesado} & \textbf{Algoritmo} & \textbf{Parámetros} \\
        \hline
        01 & 23/12/2025 20:30 & 1 & 0.8278 & 0.84782 & StandardScaler & LogisticRegression & max\_iter=1000, random\_state=42 \\
        \hline
        02 & 25/12/2025 18:46 & -- & 0.7368 & 0.74561 & 
        \parbox[t]{2.3cm}{StandardScaler + PCA(3)} & LogisticRegression & max\_iter=1000, random\_state=42 \\
        \hline
        03 & 25/12/2025 18:47 & -- & 0.7268 & 0.69364 & 
        \parbox[t]{2.3cm}{StandardScaler + PCA(7)} & XGBoost & n\_estimators=100, max\_depth=6 \\
        \hline
        04 & 25/12/2025 18:46 & -- & 0.6802 & 0.59459 & 
        \parbox[t]{2.3cm}{StandardScaler + SKB(50)} & RandomForest & n\_estimators=100, class\_weight='balanced' \\
        \hline
        05 & 25/12/2025 18:48 & -- & 0.7472 & 0.70454 & StandardScaler & XGBoost & n\_estimators=100, max\_depth=6 \\
        \hline
        06 & 25/12/2025 18:48 & -- & 0.7232 & 0.73033 & StandardScaler & LightGBM & n\_estimators=100, max\_depth=6 \\
        \hline
    \end{tabular}
    \caption{Registro obligatorio de submissions a Kaggle - Fases 1 y 2 completadas.}
    \label{tab:experimentos}
\end{table}

\subsection{Detalle del Experimento 01}

\begin{itemize}
    \item \textbf{Fecha y hora de subida}: 23/12/2025 20:30
    \item \textbf{Posición en el momento}: 1 de 2
    \item \textbf{Score en entrenamiento (CV 5-fold)}: 0.8278 ($\pm$ 0.0232)
    \item \textbf{Score en Kaggle (test)}: 0.8478
    \item \textbf{Preprocesado}:
    \begin{itemize}
        \item Limpieza de valores numéricos con espacios
        \item Exclusión de columnas de metadatos (exp, dpi, leaf, spot)
        \item Normalización con StandardScaler
        \item Sin reducción de dimensionalidad (304 features)
    \end{itemize}
    \item \textbf{Algoritmo}: Logistic Regression
    \item \textbf{Parámetros}: \texttt{max\_iter=1000, random\_state=42}
    \item \textbf{Observaciones}: Modelo baseline. El score en Kaggle (0.8478) supera ligeramente al score de validación cruzada (0.8278), indicando buena generalización.
\end{itemize}

\subsection{Fase 1: Reducción de Dimensionalidad}

El objetivo de esta fase fue evaluar el impacto de diferentes técnicas de reducción de dimensionalidad en el rendimiento del modelo. Se implementaron tres enfoques principales:

\subsubsection{Experimento 02: PCA(3) + Logistic Regression}

Se aplicó Análisis de Componentes Principales reduciendo a 3 componentes, que capturan el 95\% de la varianza total. Los resultados mostraron una disminución significativa en el rendimiento:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.7368 ($\pm$ 0.0020)
    \item \textbf{Observación}: Aunque se retiene la mayor parte de la varianza, se pierde información discriminativa crucial para la clasificación.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../graficas/pca_varianza_explicada.png}
    \caption{Varianza explicada por componentes principales.}
    \label{fig:pca_varianza}
\end{figure}

\subsubsection{Experimento 03: PCA(7) + XGBoost}

Se evaluó PCA con 7 componentes (99\% de varianza) combinado con XGBoost:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.7268 ($\pm$ 0.0508)
    \item \textbf{Observación}: Aumentar a 7 componentes no mejora significativamente respecto a PCA(3), y la alta varianza indica inestabilidad.
\end{itemize}

\subsubsection{Experimento 04: SelectKBest(50) + Random Forest}

Se seleccionaron las 50 mejores features mediante prueba F de ANOVA:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.6802 ($\pm$ 0.0544)
    \item \textbf{Observación}: Selección agresiva de features elimina información importante, resultando en el peor rendimiento.
\end{itemize}

\subsubsection{Conclusiones Fase 1}

\begin{enumerate}
    \item \textbf{La reducción de dimensionalidad no beneficia el rendimiento} en este problema específico.
    \item \textbf{PCA(3) es la mejor opción} si se requiere reducción, pero aún inferior al baseline.
    \item \textbf{SelectKBest es demasiado agresivo} con k=50 para este dataset.
    \item \textbf{Se recomienda mantener todas las features} para los modelos avanzados.
\end{enumerate}

\clearpage
\subsection{Fase 2: Modelos Avanzados}

En esta fase se implementaron algoritmos de boosting state-of-the-art para comparar con el baseline de Logistic Regression.

\subsubsection{Experimento 05: XGBoost Baseline}

XGBoost con parámetros por defecto sobre todas las features:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.7472 ($\pm$ 0.0520)
    \item \textbf{Parámetros}: n\_estimators=100, max\_depth=6, learning\_rate=0.1
    \item \textbf{Observación}: No supera al baseline, alta varianza indica necesidad de optimización.
\end{itemize}

\subsubsection{Experimento 06: LightGBM Baseline}

LightGBM como alternativa eficiente a XGBoost:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.7232 ($\pm$ 0.0556)
    \item \textbf{Parámetros}: n\_estimators=100, max\_depth=6, learning\_rate=0.1
    \item \textbf{Observación}: Rendimiento similar a XGBoost, también requiere optimización.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../graficas/comparacion_modelos_fase1.png}
    \caption{Comparación de F1-scores en validación cruzada para todos los experimentos.}
    \label{fig:comparacion_fase1}
\end{figure}

\subsubsection{Análisis de Resultados Kaggle}

Los resultados obtenidos en la competición Kaggle (Tabla \ref{tab:experimentos}) revelan patrones importantes sobre el comportamiento de los modelos en datos de test:

\begin{itemize}
    \item \textbf{Logistic Regression (Exp 01):} F1-test = 0.84782
    \begin{itemize}
        \item \textcolor{green}{Mejor resultado global}, posicionándose en 1º lugar
        \item Mejora generalización: F1-test (0.84782) $>$ F1-CV (0.8278)
        \item Modelo robusto y estable con parámetros por defecto
    \end{itemize}
    
    \item \textbf{PCA(3) + Logistic Regression (Exp 02):} F1-test = 0.74561
    \begin{itemize}
        \item \textcolor{orange}{Segundo mejor resultado}, pero 12\% inferior al baseline
        \item Ligera mejora generalización: F1-test (0.74561) $>$ F1-CV (0.7368)
        \item Confirma que PCA(3) pierde información discriminativa
    \end{itemize}
    
    \item \textbf{LightGBM (Exp 06):} F1-test = 0.73033
    \begin{itemize}
        \item \textcolor{blue}{Tercer puesto}, mejor que XGBoost en test
        \item Mejor generalización: F1-test (0.73033) $>$ F1-CV (0.7232)
        \item Potencial con optimización de hiperparámetros
    \end{itemize}
    
    \item \textbf{XGBoost (Exp 05):} F1-test = 0.70454
    \begin{itemize}
        \item \textcolor{red}{Peor generalización}: F1-test (0.70454) $<$ F1-CV (0.7472)
        \item Overfitting evidente con parámetros por defecto
        \item Requiere optimización urgente de hiperparámetros
    \end{itemize}
    
    \item \textbf{PCA(7) + XGBoost (Exp 03):} F1-test = 0.69364
    \begin{itemize}
        \item Peor que PCA(3), confirmando que más componentes no ayudan
        \item Combinación subóptima de reducción + modelo complejo
    \end{itemize}
    
    \item \textbf{SelectKBest(50) + Random Forest (Exp 04):} F1-test = 0.59459
    \begin{itemize}
        \item \textcolor{red}{Peor resultado global}, 30\% inferior al baseline
        \item Selección agresiva elimina información crítica
        \item Random Forest no se beneficia de features reducidas
    \end{itemize}
\end{itemize}

\textbf{Observaciones Clave:}
\begin{enumerate}
    \item \textbf{Simplicidad vs Complejidad:} Logistic Regression supera a modelos complejos, demostrando que para este problema un modelo lineal bien regularizado es suficiente.
    \item \textbf{Reducción Dimensional:} Ninguna técnica de reducción mejora el baseline, sugiriendo que las 304 features contienen información única y complementaria.
    \item \textbf{Generalización:} Los modelos más simples (LR, LightGBM) generalizan mejor, mientras que XGBoost sufre overfitting.
    \item \textbf{Gap Train-Test:} El mayor gap se observa en XGBoost (0.043), indicando alta varianza y necesidad de regularización.
\end{enumerate}

\subsubsection{Feature Importance}

El análisis con XGBoost (Figura \ref{fig:feature_importance}) revela que las variables espectrales en rangos específicos son más importantes que las de fluorescencia.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../graficas/feature_importance_xgboost.png}
    \caption{Top 20 features más importantes según XGBoost.}
    \label{fig:feature_importance}
\end{figure}

\subsubsection{Conclusiones Fase 2}

\begin{enumerate}
    \item \textbf{Logistic Regression sigue siendo el mejor modelo} con F1-test = 0.84782 (1º en Kaggle).
    \item \textbf{LightGBM muestra potencial} con mejor generalización que XGBoost (F1-test = 0.73033).
    \item \textbf{XGBoost requiere optimización crítica} debido a overfitting evidente (gap de 0.043).
    \item \textbf{Las features espectrales son más discriminativas} que las de fluorescencia según análisis de importancia.
    \item \textbf{Modelos complejos no garantizan mejor rendimiento} en este problema específico.
\end{enumerate}

% 6. Conclusiones
\section{Conclusiones}

\subsection{Conclusiones del EDA}

\begin{enumerate}
    \item El dataset presenta un desbalance moderado (1.40:1) que no requiere técnicas especiales de balanceo.
    \item Las variables de fluorescencia forman dos grupos correlacionados con información complementaria.
    \item Alta reducibilidad dimensional: 3 componentes PCA capturan el 95\% de la varianza.
    \item No hay valores faltantes y los outliers son escasos (<3\%).
\end{enumerate}

\subsection{Conclusiones de Modelos}

\begin{enumerate}
    \item \textbf{Logistic Regression es el modelo ganador} con F1-test = 0.84782, alcanzando el 1º lugar en Kaggle y demostrando que la simplicidad supera a la complejidad.
    \item \textbf{La reducción dimensional empeora el rendimiento}: PCA(3) reduce el F1 en 12\% (0.74561), confirmando que todas las 304 features aportan información única.
    \item \textbf{LightGBM muestra el mejor potencial entre modelos complejos} con F1-test = 0.73033 y buena generalización, superando a XGBoost.
    \item \textbf{XGBoost sufre overfitting severo} con gap de 0.043 entre CV y test, requiriendo optimización urgente de hiperparámetros.
    \item \textbf{SelectKBest es contraproducente}: eliminar features reduce drásticamente el rendimiento (F1 = 0.59459, 30\% inferior al baseline).
    \item \textbf{Las features espectrales son más discriminativas} que las de fluorescencia según análisis de importancia.
\end{enumerate}

\subsection{Lecciones Aprendidas}

\begin{enumerate}
    \item Para problemas de clasificación con datos espectroscópicos, modelos lineales bien regularizados pueden superar a ensembles complejos.
    \item La reducción dimensional no siempre mejora el rendimiento; depende de la naturaleza discriminativa de las features.
    \item La generalización es más importante que el rendimiento en validación cruzada para competiciones Kaggle.
    \item Es fundamental evaluar múltiples enfoques antes de asumir que modelos complejos son superiores.
\end{enumerate}

\clearpage
% 7. Estructura de la Entrega
\section{Estructura de la Entrega}

\begin{verbatim}
P3/
+-- data/                      # Datos originales
|   +-- train.csv
|   +-- test.csv
|   +-- sample_submission.csv
+-- notebooks/                 # Jupyter notebooks
|   +-- 01_EDA.ipynb          # Analisis exploratorio
|   +-- 02_Experimentos.ipynb # Experimentos interactivos
+-- src/                       # Codigo fuente
|   +-- preprocessing.py       # Preprocesado
|   +-- models.py             # Modelos
|   +-- utils.py              # Utilidades
+-- scripts/                   # Scripts de experimentos
|   +-- exp_01_baseline.py    # Exp 01: Baseline LR
|   +-- exp_02_pca_logistic.py # Exp 02: PCA(3) + LR
|   +-- exp_03_pca_xgboost.py  # Exp 03: PCA(7) + XGBoost
|   +-- exp_04_selectkbest_rf.py # Exp 04: SKB(50) + RF
|   +-- exp_05_xgboost_baseline.py # Exp 05: XGBoost
|   +-- exp_06_lightgbm_baseline.py # Exp 06: LightGBM
+-- submissions/               # Archivos CSV para Kaggle
|   +-- submission_01_baseline_logisticregression_*.csv
|   +-- submission_02_pca3_logistic_*.csv
|   +-- submission_03_pca7_xgboost_*.csv
|   +-- submission_04_selectkbest50_rf_*.csv
|   +-- submission_05_xgboost_baseline_*.csv
|   +-- submission_06_lightgbm_baseline_*.csv
+-- docs/                      # Documentacion
|   +-- latex/                # Fuentes LaTeX
+-- requirements.txt          # Dependencias
\end{verbatim}

% 8. Bibliografía
\section{Bibliografía}

\begin{itemize}
    \item Pedregosa, F. et al. (2011). \textit{Scikit-learn: Machine Learning in Python}. JMLR, 12, 2825--2830.
    \item Chen, T. \& Guestrin, C. (2016). \textit{XGBoost: A Scalable Tree Boosting System}. KDD'16.
    \item Ke, G. et al. (2017). \textit{LightGBM: A Highly Efficient Gradient Boosting Decision Tree}. NIPS'17.
    \item Documentación oficial de scikit-learn: \url{https://scikit-learn.org/stable/}
    \item Documentación oficial de XGBoost: \url{https://xgboost.readthedocs.io/}
    \item Documentación oficial de LightGBM: \url{https://lightgbm.readthedocs.io/}
    \item Documentación oficial de pandas: \url{https://pandas.pydata.org/}
\end{itemize}

\end{document}
