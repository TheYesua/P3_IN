\documentclass[11pt]{article}
\usepackage[left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{graphicx}
\usepackage{float}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyhdr}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{xcolor}

\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    frame=single,
    breaklines=true,
    columns=fullflexible
}

\setlength{\parindent}{0mm}
\setlength{\parskip}{1em}
\pagestyle{fancy}
\setlength{\headheight}{14pt}
\rhead{Inteligencia de Negocio}
\lhead{Práctica 3}
\renewcommand{\footrulewidth}{0.5pt}
\fancyfoot[L]{Grado en Ingeniería Informática}
\fancyfoot[C]{\thepage}
\fancyfoot[R]{\hyperref[sec:indice]{Índice}}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{\fill}
    {\Huge\bfseries Práctica 3 \\[0.5cm]}
    {\LARGE Competición Kaggle: Clasificación de Hojas de Tomate \\[0.5cm]}
    {\Large Inteligencia de Negocio \\[2cm]}
    {\large Jesús J. Cantero \\[0.3cm]}
    {\large Grupo A \\[0.3cm]}
    {\large jesusjcl@correo.ugr.es \\[2cm]}
    {\large Curso 2025--2026 \\[0.5cm]}
    \vspace*{\fill}
    {\large Grado en Ingeniería Informática \\}
    {\large Universidad de Granada (Ceuta)}
\end{titlepage}

% Página obligatoria: Captura del Leaderboard de Kaggle
\thispagestyle{empty}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{../capturas/leaderboard_01012026.png}
    \caption*{\textbf{Captura del Leaderboard de Kaggle} -- Fecha: 01/01/2026}
\end{figure}
\vspace{1cm}
\begin{center}
    \textbf{Resumen de participación:}\\[0.5cm]
    \begin{tabular}{ll}
        \textbf{Usuario:} & JesusCantero\_UGR\_IN \\
        \textbf{Posición actual:} & 2 de 5 \\
        \textbf{Mejor Score:} & 0.84782 \\
        \textbf{Número de entries:} & 10 \\
    \end{tabular}
\end{center}
\clearpage

\tableofcontents
\label{sec:indice}
\clearpage

% 1. Introducción
\section{Introducción}

\subsection{Contexto y Motivación}

Esta práctica consiste en una competición Kaggle de clasificación binaria de hojas de tomate. El objetivo es distinguir entre hojas sanas (\textit{control}) y hojas infectadas por el hongo \textit{Botrytis cinerea} (\textit{botrytis}) utilizando datos de fluorescencia multicolor e imágenes hiperespectrales.

La detección temprana de enfermedades en cultivos es fundamental para la agricultura de precisión y la gestión eficiente de recursos. Las técnicas de aprendizaje automático permiten automatizar el diagnóstico a partir de mediciones no destructivas, reduciendo pérdidas y optimizando el uso de tratamientos fitosanitarios.

\subsection{Descripción del Dataset}

El dataset proporcionado contiene mediciones de hojas de tomate:

\begin{itemize}
    \item \textbf{Conjunto de entrenamiento}: 336 muestras con etiquetas
    \item \textbf{Conjunto de test}: 143 muestras sin etiquetas
    \item \textbf{Total de variables}: 309 columnas
\end{itemize}

\begin{table}[H]
    \centering
    \begin{tabular}{lll}
        \toprule
        \textbf{Tipo} & \textbf{Columnas} & \textbf{Descripción} \\
        \midrule
        Metadatos (NO USAR) & \texttt{exp}, \texttt{dpi}, \texttt{leaf}, \texttt{spot} & Información experimental \\
        Fluorescencia & \texttt{F440}, \texttt{F520}, \texttt{F680}, \texttt{F740} & 4 valores de fluorescencia \\
        Hiperespectral & \texttt{w388.13} a \texttt{w1028.28} & 300 variables espectrales \\
        Target & \texttt{class} & \texttt{control} (0) o \texttt{botrytis} (1) \\
        \bottomrule
    \end{tabular}
    \caption{Descripción de las variables del dataset.}
    \label{tab:variables}
\end{table}

\subsection{Herramientas y Tecnologías}

\begin{itemize}
    \item \textbf{pandas} y \textbf{numpy}: manipulación de datos
    \item \textbf{scikit-learn}: modelos de clasificación, escalado y validación cruzada
    \item \textbf{XGBoost} y \textbf{LightGBM}: algoritmos de gradient boosting
    \item \textbf{matplotlib} y \textbf{seaborn}: visualización
    \item \textbf{Jupyter Notebook}: análisis exploratorio interactivo
\end{itemize}

\clearpage
\subsection{Métrica de Evaluación}

La métrica utilizada en la competición es el \textbf{F1-score}, definido como:

\[
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]

Esta métrica es apropiada para problemas de clasificación binaria con posible desbalance de clases, ya que considera tanto los falsos positivos como los falsos negativos.

\clearpage
% 2. Análisis Exploratorio de Datos
\section{Análisis Exploratorio de Datos (EDA)}

\subsection{Distribución de Clases}

El análisis de la distribución de clases revela un desbalance moderado:

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Clase} & \textbf{Muestras} & \textbf{Porcentaje} \\
        \midrule
        botrytis & 196 & 58.3\% \\
        control & 140 & 41.7\% \\
        \bottomrule
    \end{tabular}
    \caption{Distribución de clases en el conjunto de entrenamiento.}
    \label{tab:distribucion}
\end{table}

El ratio de desbalance es de 1.40:1, lo cual es moderado y no requiere técnicas agresivas de balanceo.

\subsection{Variables de Fluorescencia}

Las 4 variables de fluorescencia presentan las siguientes características:

\begin{table}[H]
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Variable} & \textbf{Media} & \textbf{Std} & \textbf{Min} & \textbf{Max} \\
        \midrule
        F440 & 235.57 & 9.19 & 209.95 & 257.33 \\
        F520 & 431.88 & 31.66 & 327.26 & 517.98 \\
        F680 & 773.10 & 120.98 & 484.27 & 1270.81 \\
        F740 & 1403.55 & 227.24 & 795.12 & 2125.33 \\
        \bottomrule
    \end{tabular}
    \caption{Estadísticas descriptivas de las variables de fluorescencia.}
    \label{tab:fluorescencia}
\end{table}

Se observan dos grupos de variables correlacionadas:
\begin{itemize}
    \item \textbf{Grupo 1}: F440 y F520 (correlación positiva: 0.89)
    \item \textbf{Grupo 2}: F680 y F740 (correlación positiva: 0.96)
\end{itemize}

Existe correlación negativa entre ambos grupos, sugiriendo características complementarias.

\subsection{Variables Espectrales}

El dataset incluye 300 variables espectrales en el rango de 388.13 nm a 1028.28 nm. El análisis de espectros promedio por clase muestra diferencias sutiles pero consistentes entre hojas sanas e infectadas.

\subsection{Análisis de Valores Faltantes y Outliers}

\begin{itemize}
    \item \textbf{Valores faltantes}: No se detectaron valores faltantes en ninguno de los conjuntos.
    \item \textbf{Outliers} (método IQR en fluorescencia):
    \begin{itemize}
        \item F440: 4 outliers (1.2\%)
        \item F520: 10 outliers (3.0\%)
        \item F680: 6 outliers (1.8\%)
        \item F740: 4 outliers (1.2\%)
    \end{itemize}
\end{itemize}

\subsection{Reducción de Dimensionalidad (PCA)}

El análisis de componentes principales revela alta reducibilidad dimensional:

\begin{itemize}
    \item \textbf{3 componentes} explican el 95\% de la varianza
    \item \textbf{7 componentes} explican el 99\% de la varianza
\end{itemize}

Esto indica que, a pesar de tener 304 features, la información relevante se concentra en pocas dimensiones.

\clearpage
% 3. Preprocesamiento
\section{Preprocesamiento de Datos}

\subsection{Limpieza de Datos}

Durante la carga de datos se detectó un problema de formato: algunos valores numéricos contenían espacios (ej: \texttt{'232 .25'}). Se implementó una función de limpieza automática:

\begin{lstlisting}
def clean_numeric_columns(df):
    for col in df.columns:
        if df[col].dtype == 'object' and col not in METADATA_COLS + [TARGET_COL]:
            df[col] = df[col].astype(str).str.replace(' ', '').astype(float)
    return df
\end{lstlisting}

\subsection{Preparación de Features}

El pipeline de preprocesamiento incluye:

\begin{enumerate}
    \item \textbf{Exclusión de metadatos}: Se eliminan las columnas \texttt{exp}, \texttt{dpi}, \texttt{leaf}, \texttt{spot}
    \item \textbf{Codificación del target}: \texttt{control} $\rightarrow$ 0, \texttt{botrytis} $\rightarrow$ 1
    \item \textbf{Escalado}: StandardScaler para normalizar las features
\end{enumerate}

\subsection{Arquitectura del Código}

El código se organiza en módulos reutilizables:

\begin{itemize}
    \item \texttt{src/preprocessing.py}: Funciones de carga, limpieza y transformación
    \item \texttt{src/models.py}: Definición y evaluación de modelos
    \item \texttt{src/utils.py}: Utilidades para submissions y logging
\end{itemize}

\clearpage
% 4. Modelado
\section{Modelado y Experimentación}

\subsection{Metodología}

Se utilizó validación cruzada estratificada con 5 folds para evaluar los modelos, asegurando que cada fold mantiene la proporción de clases del dataset original.

\subsection{Experimento 01: Modelos Baseline}

Se compararon 5 algoritmos de clasificación con parámetros por defecto:

\begin{table}[H]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Modelo} & \textbf{F1-Score (CV)} & \textbf{Desv. Std} \\
        \midrule
        Logistic Regression & \textbf{0.8278} & 0.0232 \\
        SVM (RBF) & 0.7624 & 0.0397 \\
        Random Forest & 0.7530 & 0.0510 \\
        Gradient Boosting & 0.7371 & 0.0314 \\
        KNN (k=5) & 0.6558 & 0.0681 \\
        \bottomrule
    \end{tabular}
    \caption{Comparación de modelos baseline con StandardScaler.}
    \label{tab:baseline}
\end{table}

\textbf{Resultado}: Logistic Regression obtuvo el mejor F1-score en validación cruzada (0.8278), siendo seleccionado para la primera submission.

\subsection{Configuración del Mejor Modelo}

\begin{lstlisting}
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

# Preprocesamiento
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Modelo
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train_scaled, y_train)

# Predicciones
predictions = model.predict(X_test_scaled)
\end{lstlisting}

\clearpage
% 5. Registro de Experimentos
\section{Registro de Experimentos}

La siguiente tabla contiene el registro obligatorio de todas las submissions realizadas a Kaggle, incluyendo fecha/hora, posición en el momento de la subida, scores de entrenamiento y test, preprocesado aplicado, algoritmo utilizado y configuración de parámetros.

\begin{table}[H]
    \centering
    \scriptsize
    \begin{tabular}{|c|c|c|c|c|p{1.8cm}|p{1.5cm}|p{2.2cm}|}
        \hline
        \textbf{Nº} & \textbf{Fecha} & \textbf{Pos.} & \textbf{F1 CV} & \textbf{F1 Kaggle} & \textbf{Preprocesado} & \textbf{Algoritmo} & \textbf{Descripción} \\
        \hline
        -- & 21/12/25 & -- & -- & 0.00000 & -- & -- & Prueba (sample) \\
        \hline
        01a & 23/12/25 & 1 & 0.8278 & \textbf{0.84782} & Scaler & LR & Baseline (seed=42) \\
        \hline
        01b & 23/12/25 & 1 & $\sim$0.83 & \textbf{0.84782} & Scaler & LR & Baseline (otra seed) \\
        \hline
        02 & 25/12/25 & -- & 0.7368 & 0.74561 & Scaler+PCA(3) & LR & 95\% varianza \\
        \hline
        03 & 25/12/25 & -- & 0.7268 & 0.69364 & Scaler+PCA(7) & XGBoost & 99\% varianza \\
        \hline
        04 & 25/12/25 & -- & 0.6802 & 0.59459 & Scaler+SKB(50) & RF & ANOVA F-value \\
        \hline
        05 & 25/12/25 & -- & 0.7472 & 0.70454 & Scaler & XGBoost & est=100, d=6 \\
        \hline
        06 & 25/12/25 & -- & 0.7232 & 0.73033 & Scaler & LightGBM & est=100, d=6 \\
        \hline
        15 & 01/01/26 & 2 & 0.8518 & 0.82485 & Scaler & Ridge & $\alpha$=1.0 \\
        \hline
        19 & 01/01/26 & 2 & 0.8517 & 0.84491 & Spec+RFE(100) & LR & 100 features \\
        \hline
    \end{tabular}
    \caption{Registro completo de 10 submissions a Kaggle. LR=LogisticRegression, RF=RandomForest, Scaler=StandardScaler, SKB=SelectKBest, est=n\_estimators, d=max\_depth, Spec=Spectral.}
    \label{tab:experimentos}
\end{table}

\subsection{Detalle del Experimento 01}

\begin{itemize}
    \item \textbf{Fecha y hora de subida}: 23/12/2025 20:30
    \item \textbf{Posición en el momento}: 1 de 2
    \item \textbf{Score en entrenamiento (CV 5-fold)}: 0.8278 ($\pm$ 0.0232)
    \item \textbf{Score en Kaggle (test)}: 0.84782
    \item \textbf{Preprocesado}:
    \begin{itemize}
        \item Limpieza de valores numéricos con espacios
        \item Exclusión de columnas de metadatos (exp, dpi, leaf, spot)
        \item Normalización con StandardScaler
        \item Sin reducción de dimensionalidad (304 features)
    \end{itemize}
    \item \textbf{Algoritmo}: Logistic Regression
    \item \textbf{Parámetros}: \texttt{max\_iter=1000, random\_state=42}
    \item \textbf{Observaciones}: Modelo baseline. El score en Kaggle (0.84782) supera ligeramente al score de validación cruzada (0.8278), indicando buena generalización.
\end{itemize}

\subsection{Fase 1: Reducción de Dimensionalidad}

El objetivo de esta fase fue evaluar el impacto de diferentes técnicas de reducción de dimensionalidad en el rendimiento del modelo. Se implementaron tres enfoques principales:

\subsubsection{Experimento 02: PCA(3) + Logistic Regression}

Se aplicó Análisis de Componentes Principales reduciendo a 3 componentes, que capturan el 95\% de la varianza total. Los resultados mostraron una disminución significativa en el rendimiento:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.7368 ($\pm$ 0.0020)
    \item \textbf{Observación}: Aunque se retiene la mayor parte de la varianza, se pierde información discriminativa crucial para la clasificación.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../graficas/pca_varianza_explicada.png}
    \caption{Varianza explicada por componentes principales.}
    \label{fig:pca_varianza}
\end{figure}

\subsubsection{Experimento 03: PCA(7) + XGBoost}

Se evaluó PCA con 7 componentes (99\% de varianza) combinado con XGBoost:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.7268 ($\pm$ 0.0508)
    \item \textbf{Observación}: Aumentar a 7 componentes no mejora significativamente respecto a PCA(3), y la alta varianza indica inestabilidad.
\end{itemize}

\subsubsection{Experimento 04: SelectKBest(50) + Random Forest}

Se seleccionaron las 50 mejores features mediante prueba F de ANOVA:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.6802 ($\pm$ 0.0544)
    \item \textbf{Observación}: Selección agresiva de features elimina información importante, resultando en el peor rendimiento.
\end{itemize}

\subsubsection{Conclusiones Fase 1}

\begin{enumerate}
    \item \textbf{La reducción de dimensionalidad no beneficia el rendimiento} en este problema específico.
    \item \textbf{PCA(3) es la mejor opción} si se requiere reducción, pero aún inferior al baseline.
    \item \textbf{SelectKBest es demasiado agresivo} con k=50 para este dataset.
    \item \textbf{Se recomienda mantener todas las features} para los modelos avanzados.
\end{enumerate}

\clearpage
\subsection{Fase 2: Modelos Avanzados}

En esta fase se implementaron algoritmos de boosting state-of-the-art para comparar con el baseline de Logistic Regression.

\subsubsection{Experimento 05: XGBoost Baseline}

XGBoost con parámetros por defecto sobre todas las features:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.7472 ($\pm$ 0.0520)
    \item \textbf{Parámetros}: n\_estimators=100, max\_depth=6, learning\_rate=0.1
    \item \textbf{Observación}: No supera al baseline, alta varianza indica necesidad de optimización.
\end{itemize}

\subsubsection{Experimento 06: LightGBM Baseline}

LightGBM como alternativa eficiente a XGBoost:

\begin{itemize}
    \item \textbf{F1-score CV}: 0.7232 ($\pm$ 0.0556)
    \item \textbf{Parámetros}: n\_estimators=100, max\_depth=6, learning\_rate=0.1
    \item \textbf{Observación}: Rendimiento similar a XGBoost, también requiere optimización.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{../graficas/comparacion_modelos_fase1.png}
    \caption{Comparación de F1-scores en validación cruzada para todos los experimentos.}
    \label{fig:comparacion_fase1}
\end{figure}

\subsubsection{Análisis de Resultados Kaggle}

Los resultados obtenidos en la competición Kaggle (Tabla \ref{tab:experimentos}) revelan patrones importantes sobre el comportamiento de los modelos en datos de test:

\begin{itemize}
    \item \textbf{Logistic Regression (Exp 01):} F1-test = 0.84782
    \begin{itemize}
        \item \textcolor{green}{Mejor resultado del equipo}, alcanzando el 2º lugar en el leaderboard
        \item Mejora generalización: F1-test (0.84782) $>$ F1-CV (0.8278)
        \item Modelo robusto y estable con parámetros por defecto
    \end{itemize}
    
    \item \textbf{PCA(3) + Logistic Regression (Exp 02):} F1-test = 0.74561
    \begin{itemize}
        \item \textcolor{orange}{Segundo mejor resultado}, pero 12\% inferior al baseline
        \item Ligera mejora generalización: F1-test (0.74561) $>$ F1-CV (0.7368)
        \item Confirma que PCA(3) pierde información discriminativa
    \end{itemize}
    
    \item \textbf{LightGBM (Exp 06):} F1-test = 0.73033
    \begin{itemize}
        \item \textcolor{blue}{Tercer puesto}, mejor que XGBoost en test
        \item Mejor generalización: F1-test (0.73033) $>$ F1-CV (0.7232)
        \item Potencial con optimización de hiperparámetros
    \end{itemize}
    
    \item \textbf{XGBoost (Exp 05):} F1-test = 0.70454
    \begin{itemize}
        \item \textcolor{red}{Peor generalización}: F1-test (0.70454) $<$ F1-CV (0.7472)
        \item Overfitting evidente con parámetros por defecto
        \item Requiere optimización urgente de hiperparámetros
    \end{itemize}
    
    \item \textbf{PCA(7) + XGBoost (Exp 03):} F1-test = 0.69364
    \begin{itemize}
        \item Peor que PCA(3), confirmando que más componentes no ayudan
        \item Combinación subóptima de reducción + modelo complejo
    \end{itemize}
    
    \item \textbf{SelectKBest(50) + Random Forest (Exp 04):} F1-test = 0.59459
    \begin{itemize}
        \item \textcolor{red}{Peor resultado global}, 30\% inferior al baseline
        \item Selección agresiva elimina información crítica
        \item Random Forest no se beneficia de features reducidas
    \end{itemize}
\end{itemize}

\textbf{Observaciones Clave:}
\begin{enumerate}
    \item \textbf{Simplicidad vs Complejidad:} Logistic Regression supera a modelos complejos, demostrando que para este problema un modelo lineal bien regularizado es suficiente.
    \item \textbf{Reducción Dimensional:} Ninguna técnica de reducción mejora el baseline, sugiriendo que las 304 features contienen información única y complementaria.
    \item \textbf{Generalización:} Los modelos más simples (LR, LightGBM) generalizan mejor, mientras que XGBoost sufre overfitting.
    \item \textbf{Gap Train-Test:} El mayor gap se observa en XGBoost (0.043), indicando alta varianza y necesidad de regularización.
\end{enumerate}

\subsubsection{Feature Importance}

El análisis con XGBoost (Figura \ref{fig:feature_importance}) revela que las variables espectrales en rangos específicos son más importantes que las de fluorescencia.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{../graficas/feature_importance_xgboost.png}
    \caption{Top 20 features más importantes según XGBoost.}
    \label{fig:feature_importance}
\end{figure}

\subsubsection{Conclusiones Fase 2}

\begin{enumerate}
    \item \textbf{Logistic Regression sigue siendo el mejor modelo} con F1-test = 0.84782 (2º en Kaggle).
    \item \textbf{LightGBM muestra potencial} con mejor generalización que XGBoost (F1-test = 0.73033).
    \item \textbf{XGBoost requiere optimización crítica} debido a overfitting evidente (gap de 0.043).
    \item \textbf{Las features espectrales son más discriminativas} que las de fluorescencia según análisis de importancia.
    \item \textbf{Modelos complejos no garantizan mejor rendimiento} en este problema específico.
\end{enumerate}

\clearpage
\subsection{Fase 3: Optimización del Modelo}

Tras confirmar que Logistic Regression era el mejor modelo, se realizaron múltiples experimentos de optimización para intentar superar el score de 0.84782 y alcanzar al líder (0.85561).

\subsubsection{Experimentos de Optimización Realizados}

Se implementaron 13 experimentos adicionales (Exp07-Exp19), de los cuales los más relevantes fueron:

\begin{table}[H]
    \centering
    \begin{tabular}{llccc}
        \toprule
        \textbf{Exp} & \textbf{Descripción} & \textbf{F1-CV} & \textbf{F1-Kaggle} & \textbf{Subido} \\
        \midrule
        07 & LR Optimizado (C=5, liblinear) & 0.8484 & -- & No \\
        08 & VotingClassifier (LR+SVM+RF+NB) & 0.7742 & -- & No \\
        09 & StackingClassifier & 0.8245 & -- & No \\
        10 & SVM Linear optimizado & 0.8388 & -- & No \\
        15 & Ridge Classifier ($\alpha$=1) & 0.8518 & 0.82485 & Sí \\
        18 & LR + RFE(150) todas features & 0.8479 & -- & No \\
        19 & LR + RFE(100) solo espectrales & 0.8517 & 0.84491 & Sí \\
        \bottomrule
    \end{tabular}
    \caption{Experimentos de optimización más relevantes de la Fase 3. Solo Exp15 y Exp19 fueron subidos a Kaggle.}
    \label{tab:fase3}
\end{table}

\subsubsection{Análisis de Resultados}

\textbf{Hallazgo 1: El baseline es muy estable.} Se probaron múltiples variaciones del modelo LR (diferentes seeds, valores de C, solvers) y todas producían predicciones casi idénticas (99 muestras clasificadas como botrytis).

\textbf{Hallazgo 2: Mayor F1-CV no implica mayor F1-Kaggle.} Ridge Classifier obtuvo el mejor F1-CV (0.8518), pero en Kaggle solo alcanzó 0.82485. Esto indica que el modelo baseline tiene características de generalización únicas.

\textbf{Hallazgo 3: Las features espectrales son más discriminativas.}
\begin{itemize}
    \item Solo fluorescencia (4 features): F1-CV = 0.7221
    \item Solo espectrales (300 features): F1-CV = 0.8366
    \item Todas las features (304): F1-CV = 0.8278
\end{itemize}

\textbf{Hallazgo 4: RFE mejora el F1-CV pero no el F1-Kaggle.} La selección de las 100 mejores features espectrales mediante RFE aumentó el F1-CV a 0.8517, pero en Kaggle obtuvo 0.84491 (inferior al baseline).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{../graficas/comparacion_todos_experimentos.png}
    \caption{Comparación de F1-CV de todos los experimentos por fase.}
    \label{fig:comparacion_todos}
\end{figure}

\subsubsection{Conclusiones Fase 3}

\begin{enumerate}
    \item \textbf{El baseline LR simple sigue siendo el mejor modelo en Kaggle} (F1 = 0.84782), a pesar de que otros modelos obtienen mejor F1-CV.
    \item \textbf{Existe una paradoja CV-Kaggle}: modelos con mejor validación cruzada no necesariamente generalizan mejor al test de Kaggle.
    \item \textbf{La simplicidad del baseline es su fortaleza}: menos parámetros significa menos riesgo de sobreajuste a patrones específicos del train.
    \item \textbf{El gap con el líder (0.85561 - 0.84782 = 0.00779)} es muy pequeño y probablemente requiere técnicas más avanzadas o información adicional.
\end{enumerate}

% 6. Conclusiones
\section{Conclusiones}

\subsection{Conclusiones del EDA}

\begin{enumerate}
    \item El dataset presenta un desbalance moderado (1.40:1) que no requiere técnicas especiales de balanceo.
    \item Las variables de fluorescencia forman dos grupos correlacionados con información complementaria.
    \item Alta reducibilidad dimensional: 3 componentes PCA capturan el 95\% de la varianza.
    \item No hay valores faltantes y los outliers son escasos (<3\%).
\end{enumerate}

\subsection{Conclusiones de Modelos}

\begin{enumerate}
    \item \textbf{Logistic Regression es el modelo ganador} con F1-test = 0.84782, alcanzando el 2º lugar en Kaggle (de 5 participantes) y demostrando que la simplicidad supera a la complejidad.
    \item \textbf{La reducción dimensional empeora el rendimiento}: PCA(3) reduce el F1 en 12\% (0.74561), confirmando que todas las 304 features aportan información única.
    \item \textbf{La optimización de hiperparámetros no mejoró el resultado en Kaggle}: a pesar de obtener mejor F1-CV (hasta 0.8518 con Ridge), el F1-Kaggle fue inferior al baseline.
    \item \textbf{Existe una paradoja CV-Kaggle}: el modelo más simple (LR default) generaliza mejor que modelos optimizados con mejor validación cruzada.
    \item \textbf{Las features espectrales son más discriminativas} (F1-CV = 0.8366) que las de fluorescencia (F1-CV = 0.7221).
    \item \textbf{El gap con el líder es mínimo}: solo 0.00779 puntos separan el 2º del 1º lugar.
\end{enumerate}

\subsection{Lecciones Aprendidas}

\begin{enumerate}
    \item \textbf{Simplicidad sobre complejidad}: Para problemas de clasificación con datos espectroscópicos, modelos lineales simples pueden superar a ensembles complejos y modelos optimizados.
    \item \textbf{Validación cruzada no es garantía}: Un mejor F1-CV no implica mejor F1 en test real. La generalización depende de factores no capturados por CV.
    \item \textbf{El baseline como referencia sólida}: Siempre evaluar el modelo más simple primero; a menudo es difícil de superar.
    \item \textbf{Análisis exhaustivo de features}: Identificar qué grupos de features aportan más información (espectrales vs fluorescencia) ayuda a entender el problema.
    \item \textbf{Documentar todos los experimentos}: Incluso los fallidos aportan conocimiento valioso sobre el comportamiento del dataset.
\end{enumerate}

\clearpage
% 7. Estructura de la Entrega
\section{Estructura de la Entrega}

\begin{verbatim}
P3/
+-- data/                      # Datos originales
|   +-- train.csv
|   +-- test.csv
|   +-- sample_submission.csv
+-- notebooks/                 # Jupyter notebooks
|   +-- 01_EDA.ipynb          # Analisis exploratorio
|   +-- 02_Experimentos.ipynb # Experimentos Fase 1-2
|   +-- 03_Experimentos_Fase2.ipynb # Experimentos Fase 3
+-- src/                       # Codigo fuente
|   +-- preprocessing.py       # Preprocesado
|   +-- models.py             # Modelos
|   +-- utils.py              # Utilidades
+-- scripts/                   # Scripts de experimentos
|   +-- exp_01_baseline.py    # Baseline LR (mejor resultado)
|   +-- exp_02_pca_logistic.py # PCA(3) + LR
|   +-- exp_03_pca_xgboost.py  # PCA(7) + XGBoost
|   +-- exp_04_selectkbest_rf.py # SKB(50) + RF
|   +-- exp_05_xgboost_baseline.py # XGBoost
|   +-- exp_06_lightgbm_baseline.py # LightGBM
|   +-- exp_07_lr_optimized.py # LR optimizado (C=5)
|   +-- exp_15_ridge_optimized.py # Ridge Classifier
|   +-- exp_18_feature_analysis.py # Analisis de features
|   +-- exp_19_optimized_final.py # Spectral RFE(100)
+-- submissions/               # Archivos CSV para Kaggle (10 entries)
+-- docs/                      # Documentacion
|   +-- latex/                # Fuentes LaTeX
|   +-- graficas/             # Graficas generadas
|   +-- capturas/             # Capturas del leaderboard
+-- requirements.txt          # Dependencias
\end{verbatim}

% 8. Bibliografía
\section{Bibliografía}

\begin{itemize}
    \item Pedregosa, F. et al. (2011). \textit{Scikit-learn: Machine Learning in Python}. JMLR, 12, 2825--2830.
    \item Chen, T. \& Guestrin, C. (2016). \textit{XGBoost: A Scalable Tree Boosting System}. KDD'16.
    \item Ke, G. et al. (2017). \textit{LightGBM: A Highly Efficient Gradient Boosting Decision Tree}. NIPS'17.
    \item Documentación oficial de scikit-learn: \url{https://scikit-learn.org/stable/}
    \item Documentación oficial de XGBoost: \url{https://xgboost.readthedocs.io/}
    \item Documentación oficial de LightGBM: \url{https://lightgbm.readthedocs.io/}
    \item Documentación oficial de pandas: \url{https://pandas.pydata.org/}
\end{itemize}

\end{document}
